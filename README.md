# Open Doors Archive Processor

This application, also known as the "sausage machine", uses MySQL to process legacy archives into a standardised 
set of tables which can then easily be imported into Archive of Our Own.

## Pre-requisites
- Python 3.7
- MySQL 5.x

You will also need plenty of disk space as the scripts will perform regular backups of its working files to allow you to restart the process from any point without losing the result of previous stages. How much will depend on the size of the archive you're processing.

## Quick start to process an archive
1. Make sure you read this document
1. Run 
   ```bash
   python3 start.py CODENAME PATH-TO-WORKING-DIRECTORY
   ``` 
    where:
    
    - `python3` is the path to your Python 3.7 interpreter 
    - `CODENAME` is the short name for the archive you're processing
    - `PATH-TO-WORKING-DIRECTORY` is the root directory where you want the working files to go (a subdirectory named after the CODENAME above will be created in this directory).
1. Follow the instructions on screen.

If you have already started processing the archive, you will be asked if you want to rerun any completed steps from scratch. This can be very useful if you need to edit the files from a previous step to achieve better results (for example, fixing typos).

## High-level overview of the different stages
### Stage 1 - Convert data from original archive into a standard working format
The **original** archive's metadata is processed into a standardised **working** format. This involves the following steps:
- copy authors into the `authors` table and tidy up author email addresses
- copy stories into the `stories` table and flag those that might be story links (to be imported as bookmarks)
- copy all tags into a table called `tags`
- load raw chapter contents into the `chapters` table
- create join tables to relate all the stories with their authors and metadata

The sausage machine can do this step for most archives based on eFiction, and some archives based on Automated Archive. Other types of archives will require bespoke scripts for this step - some examples of useful scripts are provided in archive_types/other.

### Stage 2 - Process the metadata to facilitate importing
This stage usually involves the following steps, which may require manual intervention:
* Clean up chapter contents:
    * Remove unwanted HTML
    * Remove broken external links
    * Fix the paths of any embedded images
    * Fix encoding errors for non-ASCII characters
    * Split chapters that are too large for the Archive
    
* Map original tags to AO3 tags:
    * Export a list of all the tags in the archive
    * Import the updated list once it has been populated by AO3 tag wranglers
    
### Stage 3 - Export the **final** database and import
The import is typically performed by loading the **final** database into a "temporary" website which is connected to the Archive's mass import API.

## Processing an archive
When you start processing a new archive, you will be prompted for the following:
* A short **code name** for your archive with no spaces or punctuation. This will be used in all the folders, files and MySQL tables generated by the process.
* For eFiction and Automated Archive archives, the full path to the original database dump (the original file will be copied before anything is done to it)
* The path to a working directory, though you can accept the default, which is to use a folder called `otw_opendoors` in your home directory.
* The username and password to use to connect to your local MySQL instance.

